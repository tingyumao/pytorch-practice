{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch RNN experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, output_size):\n",
    "        # inherit the parent's initialization without knowing parent's name\n",
    "        super().__init__()\n",
    "        # add itself new features\n",
    "        self.hidden_size = hidden_size\n",
    "        input_size = data_size + hidden_size\n",
    "        \n",
    "        # define nn functions or transform matrix\n",
    "        # here is the example given by pytorch website. \n",
    "        # However, I think it only includes two linear transform matrix in a RNN cell\n",
    "        # and it is not sufficient. A non-linear activation function should be added.\n",
    "        # A rnn cell should look like: h = tanh(W_i2h * input), output = W_h2o * h.\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, data, last_hidden):\n",
    "        # last_hidden implys the hidden state from last iteration\n",
    "        indata = torch.cat([data, last_hidden], 1)\n",
    "        hidden = self.i2h(indata)\n",
    "        output = self.h2o(hidden)\n",
    "        \n",
    "        return hidden, output\n",
    "    \n",
    "rnn = RNN(50, 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "batch_size = 10\n",
    "time_steps = 5\n",
    "\n",
    "# Create some fake data for testing\n",
    "batch = Variable(torch.randn(batch_size, 50))\n",
    "hidden = Variable(torch.zeros(batch_size, 20))\n",
    "target = Variable(torch.zeros(batch_size, 10))\n",
    "\n",
    "loss = 0.0\n",
    "for t in range(time_steps):\n",
    "    hidden, output = rnn(batch, hidden)\n",
    "    loss += loss_fn(output, target)\n",
    "    \n",
    "# backward\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.6244\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## lstm\n",
    "\n",
    "Pytorch provide LSTMCell. Also it provide other RNN cell, like GRUCell. Also it provides layer-level modules like nn.RNN, nn.LSTM, nn.LSTM. However, for later social-LSTM development, here use LSTMCell as example. Also here LSTM network is used to implement XOR function.\n",
    "\n",
    "Useful Link:\n",
    "   * RNN module and functions: http://pytorch.org/docs/master/nn.html#lstmcell\n",
    "   * optimizers: http://pytorch.org/docs/master/optim.html\n",
    "   * loss: http://pytorch.org/docs/master/nn.html#crossentropyloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        # the output of LSTMCell is h and c.\n",
    "        self.lstm = nn.LSTMCell(data_size, hidden_size)\n",
    "        # define a [output_size, hidden_size] linear mapping matrix h2o.\n",
    "        # transform h into o [batch_size, output_size] by h2o.\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, indata, h, c, time_steps):\n",
    "        \"\"\"\n",
    "        define forward function. With different time_steps, \n",
    "        we can generate prediction sequence of different length.\n",
    "        \n",
    "        Inputs:\n",
    "        - indata: [batch_size, time_steps, data_size]\n",
    "        - h: [batch_size, hidden_size]\n",
    "        - c: [batch_size, hidden_size]\n",
    "        - time_steps: for Backpropagation through time(BPTT). Make it a finite-layers network.\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        for i in range(time_steps):\n",
    "            h, c = self.lstm(indata[:,i,:], (h, c))\n",
    "            output.append(self.h2o(h))\n",
    "        # stack o together along dim=1 and get the final output with shape [batch_size, time_steps, output_size].\n",
    "        return torch.stack(output, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xor(num_samples):\n",
    "    # randomly generate 12-bits sequence\n",
    "    data = [\"{0:012b}\".format(np.random.randint(0,2**12-1)) for i in range(num_samples)]\n",
    "    shuffle(data)\n",
    "    data = [list(map(int,i)) for i in data]\n",
    "    data = np.array(data)\n",
    "    data = data.reshape(num_samples,12,1)\n",
    " \n",
    "    output = np.zeros([num_samples,12],dtype=np.int)\n",
    "    for sample,out in zip(data,output):\n",
    "        count = 0\n",
    "        for c,bit in enumerate(sample):\n",
    "            if bit[0]==1:\n",
    "                count += 1\n",
    "            out[c] = 1 - int(count%2==0)\n",
    "    return data, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 33.288563, acc: 0.520833.\n",
      "epoch: 20, loss: 33.064865, acc: 0.517361.\n",
      "epoch: 30, loss: 32.018353, acc: 0.565972.\n",
      "epoch: 40, loss: 27.318220, acc: 0.626736.\n",
      "epoch: 50, loss: 3.587850, acc: 1.000000.\n",
      "epoch: 60, loss: 0.384672, acc: 1.000000.\n",
      "epoch: 70, loss: 0.091834, acc: 1.000000.\n",
      "epoch: 80, loss: 0.043020, acc: 1.000000.\n",
      "epoch: 90, loss: 0.032155, acc: 1.000000.\n",
      "epoch: 100, loss: 0.026772, acc: 1.000000.\n"
     ]
    }
   ],
   "source": [
    "data_size = 1\n",
    "hidden_size = 64 # use 64 lstm cells\n",
    "output_size = 2\n",
    "\n",
    "net = LSTM(data_size, hidden_size, output_size)\n",
    "\n",
    "time_steps = 12\n",
    "batch_size = 48\n",
    "\n",
    "# optim\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.1)\n",
    "\n",
    "# loss function\n",
    "# inputs: input(N,C), target(N,) with int values from 0 to C-1 \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.MSELoss() \n",
    "\n",
    "# train network\n",
    "for e in range(100):\n",
    "    \n",
    "    # randomly generate a batch of data\n",
    "    # data: [batch_size, times_step, data_size]\n",
    "    # target: [batch_size, times_step]\n",
    "    data, target = xor(batch_size)\n",
    "    data = Variable(torch.from_numpy(data).float())\n",
    "    target = Variable(torch.from_numpy(target))\n",
    "    \n",
    "    # in every epoch, we need to reassign these Variables.\n",
    "    # hidden: hidden state in LSTM, cell: cell state in LSTM.\n",
    "    hidden = Variable(torch.zeros(batch_size, hidden_size))\n",
    "    cell = Variable(torch.zeros(batch_size, hidden_size))\n",
    "    \n",
    "    output = net(data, hidden, cell, time_steps)\n",
    "    \n",
    "    # here we need a for loop. And it seems that nn.CrossEntropyLoss only allows\n",
    "    # 2-dimensional input like (N,C). Also we can use tensor.view() to reshape tensor first.\n",
    "    # But I haven't tested the latter method yet.\n",
    "    loss = 0.0\n",
    "    for i in range(batch_size):\n",
    "        loss += loss_fn(output[i], target[i])\n",
    "    \n",
    "    # torch.max returns max values and indices along a specified dimension.\n",
    "    _, prediction = torch.max(output, -1)\n",
    "    acc = prediction.eq(target).float().sum() / (batch_size*time_steps)\n",
    "    \n",
    "    if (e+1)%10 == 0:\n",
    "        print(\"epoch: {}, loss: {:4f}, acc: {:4f}.\".format(e+1, loss.data.numpy()[0], acc.data.numpy()[0]))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# test phase\n",
    "batch_size = 100\n",
    "data, target = xor(batch_size)\n",
    "data = Variable(torch.from_numpy(data).float())\n",
    "target = Variable(torch.from_numpy(target))\n",
    "\n",
    "# in every epoch, we need to reassign these Variables.\n",
    "# hidden: hidden state in LSTM, cell: cell state in LSTM.\n",
    "hidden = Variable(torch.zeros(batch_size, hidden_size))\n",
    "cell = Variable(torch.zeros(batch_size, hidden_size))\n",
    "\n",
    "output = net(data, hidden, cell, time_steps)\n",
    "\n",
    "_, prediction = torch.max(output, -1)\n",
    "test_acc = prediction.eq(target).float().sum() / (batch_size*time_steps)\n",
    "print(\"test accuracy: {}\".format(test_acc.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
